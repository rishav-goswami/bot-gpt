# BotGPT - RAG Chatbot

An asynchronous, event-driven chatbot with "Chat with PDFs" capability. Built with **FastAPI**, **LangChain**, **LangGraph**, **Celery**, **Redis**, and **PostgreSQL (pgvector)**.

## ğŸš€ Key Features

* **RAG Pipeline:** Asynchronous PDF ingestion using Celery workers
* **Vector Search:** HNSW indexing via `pgvector` for efficient similarity search
* **Real-Time Updates:** Socket.IO integration for instant message delivery
* **Cost Optimized:** Deduplication of embeddings (file hashing) and sliding window context
* **Robust Parsing:** Uses `PyMuPDF` for handling complex PDF layouts
* **LLM Orchestration:** LangGraph workflow for intelligent routing between RAG and general chat
* **Multiple LLM Support:** OpenAI, Google Gemini, Groq, and Ollama

## ğŸ› ï¸ Tech Stack

* **Backend:** FastAPI, Python 3.11+
* **Database:** PostgreSQL 16 + pgvector extension
* **Async Queue:** Celery + Redis
* **LLM Orchestration:** LangChain + LangGraph
* **Real-Time:** Socket.IO with Redis adapter
* **Containerization:** Docker Compose
* **Testing:** pytest, pytest-asyncio, httpx

## ğŸ“‹ Prerequisites

* Docker and Docker Compose
* (Optional) LLM API keys (OpenAI, Groq, Google, etc.)

## ğŸƒâ€â™‚ï¸ Quick Start

### 1. Clone the Repository
```bash
git clone <repo_url>
cd bot-gpt
```

### 2. Configure Environment Variables

Create a `.env` file in the `backend/` directory (optional - defaults are provided):

```ini
# Database
DATABASE_URL=postgresql://app_user:app_password@db:5432/botgpt_db

# LLM Provider (options: openai, google, groq, ollama)
LLM_PROVIDER=groq

# API Keys (at least one required based on LLM_PROVIDER)
OPENAI_API_KEY=sk-...
GROQ_API_KEY=gsk_...
GOOGLE_API_KEY=...

# Embeddings (defaults to OpenAI if OPENAI_API_KEY is set, else HuggingFace)
OPENAI_API_KEY=sk-...  # For embeddings
EMBEDDING_MODEL=text-embedding-3-small

# Redis
REDIS_URL=redis://redis:6379/0
CELERY_BROKER_URL=redis://redis:6379/0

# CORS Origins (comma-separated)
BACKEND_CORS_ORIGINS=http://localhost:3000,http://localhost:8000
```

### 3. Start the Application

Using Docker Compose:
```bash
docker compose up --build
```

Or using Makefile:
```bash
make up
```

### 4. Access the Application

* **API Documentation:** [http://localhost:8000/docs](http://localhost:8000/docs)
* **ReDoc:** [http://localhost:8000/redoc](http://localhost:8000/redoc)
* **Health Check:** [http://localhost:8000/health](http://localhost:8000/health)
* **Test Client:** Open `test.html` in your browser

## ğŸ“¡ API Endpoints

### Health & Status

* `GET /` - Root endpoint with API information
* `GET /health/live` - Liveness probe (Kubernetes-style)
* `GET /health` - Full health check (includes database connectivity)

### Conversations

* `POST /api/v1/conversations/` - Create a new conversation
* `GET /api/v1/conversations/` - List all conversations (with pagination)
* `GET /api/v1/conversations/{chat_id}` - Get conversation details
* `POST /api/v1/conversations/{chat_id}/messages` - Send a message
* `DELETE /api/v1/conversations/{chat_id}` - Delete a conversation

### Documents

* `POST /api/v1/documents/` - Upload a PDF document
* `GET /api/v1/documents/{conversation_id}` - List documents for a conversation

### WebSocket (Socket.IO)

* Connect to `/socket.io` for real-time message updates
* Events:
  - `connect` - Connection established
  - `join_conversation` - Join a conversation room
  - `new_message` - Receive new messages
  - `doc_processed` - Document processing complete

## ğŸ§ª Testing

### Running Tests

The project includes a comprehensive test suite with 20+ tests covering all API endpoints, error handling, and edge cases.

**Option 1: Using Docker (Recommended)**
```bash
docker exec botgpt_api poetry run pytest app/tests/test_api.py -v
```

**Option 2: Inside Container Shell**
```bash
# Enter the container
docker exec -it botgpt_api /bin/bash

# Run tests
poetry run pytest app/tests/test_api.py -v

# Run specific test
poetry run pytest app/tests/test_api.py::test_create_conversation -v

# Run with coverage (if installed)
poetry run pytest app/tests/test_api.py --cov=app --cov-report=html
```

### Test Coverage

The test suite includes:
* âœ… Health endpoint tests
* âœ… Conversation CRUD operations
* âœ… Message sending and receiving
* âœ… Document upload and validation
* âœ… Error handling (404, 400, validation errors)
* âœ… Pagination
* âœ… Edge cases

All external dependencies (LLM, SocketIO, Celery) are properly mocked to ensure fast and reliable tests.

## ğŸ§ª Testing the RAG Flow (Manual)

1. Open `test.html` in your browser
2. Create a **New Chat** by sending your first message
3. Upload a PDF document (Resume, Manual, etc.)
4. Wait for the Socket.IO event: *"PDF Processed!"*
5. Ask questions specific to that PDF
6. The system will automatically use RAG when documents are attached

## ğŸ—ï¸ Architecture

### Workflow

1. **User sends message** â†’ Saved to database
2. **LangGraph workflow**:
   - Checks if conversation has documents
   - If yes: Retrieves relevant chunks using vector search
   - Generates response using RAG context
   - If no: Generates general conversation response
3. **Response saved** â†’ Emitted via Socket.IO

### Document Processing

1. **Upload** â†’ PDF saved to disk
2. **Celery task triggered** â†’ Background processing
3. **PDF parsed** â†’ Text extracted using PyMuPDF
4. **Chunking** â†’ Recursive text splitter (1000 chars, 200 overlap)
5. **Embedding** â†’ Generated using OpenAI or HuggingFace
6. **Storage** â†’ Vectors stored in PostgreSQL with pgvector
7. **Deduplication** â†’ File hash checking prevents reprocessing

## ğŸ“‚ Project Structure

```
bot-gpt/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/              # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ endpoints/
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ chats.py      # Conversation endpoints
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ documents.py  # Document endpoints
â”‚   â”‚   â”œâ”€â”€ core/             # Core configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py      # Settings management
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py   # Database connection
â”‚   â”‚   â”‚   â””â”€â”€ celery_app.py # Celery configuration
â”‚   â”‚   â”œâ”€â”€ crud/              # Database operations
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â””â”€â”€ document.py
â”‚   â”‚   â”œâ”€â”€ db/                # Database models
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py      # SQLAlchemy models
â”‚   â”‚   â”‚   â””â”€â”€ base.py        # Base classes
â”‚   â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_service.py      # RAG processing
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_graph.py        # LangGraph workflow
â”‚   â”‚   â”‚   â”œâ”€â”€ socketio_manager.py # Real-time updates
â”‚   â”‚   â”‚   â””â”€â”€ prompts.py          # Prompt management
â”‚   â”‚   â”œâ”€â”€ workers/           # Celery tasks
â”‚   â”‚   â”‚   â””â”€â”€ tasks.py        # Background jobs
â”‚   â”‚   â”œâ”€â”€ schemas/           # Pydantic models
â”‚   â”‚   â”œâ”€â”€ middlewares/       # Custom middlewares
â”‚   â”‚   â”œâ”€â”€ tests/             # Test suite
â”‚   â”‚   â”‚   â”œâ”€â”€ test_api.py    # API tests
â”‚   â”‚   â”‚   â””â”€â”€ conftest.py    # Test configuration
â”‚   â”‚   â”œâ”€â”€ main.py            # FastAPI application
â”‚   â”‚   â””â”€â”€ llm_client.py      # LLM client factory
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ pyproject.toml         # Poetry dependencies
â”‚   â””â”€â”€ uploads/               # Uploaded PDFs
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ nginx/
â”‚   â””â”€â”€ nginx.conf
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

## ğŸ› ï¸ Development

### Makefile Commands

```bash
make up      # Start all services
make down    # Stop all services
make logs    # View logs (backend + worker)
make clean   # Stop services and remove volumes
make shell   # Open shell in API container
```

### Adding New Features

1. **New API Endpoint:**
   - Add route in `app/api/v1/endpoints/`
   - Add schema in `app/schemas/`
   - Add CRUD operations in `app/crud/`
   - Add tests in `app/tests/test_api.py`

2. **New Service:**
   - Create file in `app/services/`
   - Import and use in endpoints

3. **New Model:**
   - Add model in `app/db/models.py`
   - Create migration (if using Alembic)
   - Add CRUD operations

### Code Style

* Follow PEP 8
* Use type hints
* Async/await for I/O operations
* Proper error handling with HTTPException

## ğŸ”§ Configuration

### LLM Providers

The system supports multiple LLM providers. Set `LLM_PROVIDER` in environment:

- **openai**: Uses GPT models (requires `OPENAI_API_KEY`)
- **google**: Uses Gemini (requires `GOOGLE_API_KEY`)
- **groq**: Uses Llama models (requires `GROQ_API_KEY`) - Fast and free tier available
- **ollama**: Local models (requires `OLLAMA_BASE_URL`)

### Embeddings

- **OpenAI**: If `OPENAI_API_KEY` is set, uses `text-embedding-3-small` (1536 dims)
- **HuggingFace**: Falls back to `sentence-transformers/all-MiniLM-L6-v2` (384 dims) if no OpenAI key

## ğŸ› Troubleshooting

### Database Connection Issues

```bash
# Check if database is running
docker ps | grep botgpt_db

# Check database logs
docker logs botgpt_db

# Verify connection string in docker-compose.yml
```

### Celery Worker Not Processing

```bash
# Check worker logs
docker logs botgpt_worker

# Verify Redis connection
docker exec botgpt_redis redis-cli ping
```

### Tests Failing

```bash
# Ensure database is running
docker compose up -d db

# Run tests with verbose output
docker exec botgpt_api poetry run pytest app/tests/test_api.py -v -s
```

## ğŸ“ License

[Add your license here]

## ğŸ‘¥ Contributors

* Rishav Anand - [GitHub](https://github.com/rishav-goswami)

## ğŸ™ Acknowledgments

* FastAPI for the excellent async framework
* LangChain for LLM orchestration
* pgvector for PostgreSQL vector extension
* All open-source contributors
